# DPO Fine-Tuning Pipeline (Docker + Llama Factory)

This repository provides a containerized workflow for fine-tuning Large Language Models (LLMs) using **Direct Preference Optimization (DPO)**. It utilizes [Llama Factory](https://github.com/hiyouga/LLaMA-Factory) for the training backend and wraps the process in a clean Docker environment to ensure reproducibility.

## ðŸ“‚ Project Structure

```text
.
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ train.yaml          # Hyperparameters for DPO training
â”‚   â””â”€â”€ test.yaml           # Settings for inference/testing
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset_info.json   # Auto-generated registry for Llama Factory
â”‚   â””â”€â”€ (train.json/test.json) # Generated data files
â”œâ”€â”€ .env                    # Secrets (HF Token, WandB Key)
â”œâ”€â”€ Dockerfile              # Environment definition
â”œâ”€â”€ prepare_data.py         # Script to download & format datasets
â”œâ”€â”€ train_model.py          # Wrapper to run training/testing commands
â””â”€â”€ README.md


docker build -t dpo-v1 .


# This runs prepare_data.py inside the container
docker run --rm -v $(pwd):/app dpo-v1 python prepare_data.py --all



# This runs train_model.py which triggers llamafactory-cli
docker run --gpus all --ipc=host --rm --env-file .env -v $(pwd):/app dpo-v1 python train_model.py --config configs/train.yaml



docker run --gpus all --ipc=host --rm --env-file .env -v $(pwd):/app dpo-v1 python train_model.py --config configs/test.yaml


docker run --gpus all --ipc=host --rm --env-file .env -v $(pwd):/app dpo-v1 python influence.py